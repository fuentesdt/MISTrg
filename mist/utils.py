import pynvml
import numpy as np

import tensorflow as tf
import tensorflow.keras.backend as K

def merge_two_dicts(x, y):
    z = x.copy()   # start with keys and values of x
    z.update(y)    # modifies z with keys and values of y
    return z

def auto_select_gpu():
    pynvml.nvmlInit()
    deviceCount = pynvml.nvmlDeviceGetCount()
    largest_free_mem = 0
    largest_free_idx = 0
    for i in range(deviceCount):
        handle = pynvml.nvmlDeviceGetHandleByIndex(i)
        info = pynvml.nvmlDeviceGetMemoryInfo(handle)
        if info.free > largest_free_mem:
            largest_free_mem = info.free
            largest_free_idx = i
    pynvml.nvmlShutdown()
    largest_free_mem = largest_free_mem

    idx_to_gpu_id = {}
    for i in range(deviceCount):
        idx_to_gpu_id[i] = '{}'.format(i)

    gpu_id = idx_to_gpu_id[largest_free_idx]
    print("selected GPU:", gpu_id)
    return gpu_id, largest_free_mem/1024.**3


def get_nearest_power(n):
    lower_power = 2**np.floor(np.log2(n))
    higher_power = 2**np.ceil(np.log2(n))
    
    lower_diff = np.abs(n - lower_power)
    higher_diff = np.abs(n - higher_power)
    
    if lower_diff > higher_diff:
        nearest_power = higher_power
    elif lower_diff < higher_diff:
        nearest_power = lower_power
    else:
        nearest_power = lower_power
    
    return int(nearest_power)

def get_model_memory_usage(batch_size, model):
    shapes_mem_count = 0
    internal_model_mem_count = 0
    for l in model.layers:
        layer_type = l.__class__.__name__
        if layer_type == 'Model':
            internal_model_mem_count += get_model_memory_usage(batch_size, l)
        single_layer_mem = 1
        out_shape = l.output_shape
        if type(out_shape) is list:
            out_shape = out_shape[0]
        for s in out_shape:
            if s is None:
                continue
            single_layer_mem *= s
        shapes_mem_count += single_layer_mem

    trainable_count = np.sum([K.count_params(p) for p in model.trainable_weights])
    non_trainable_count = np.sum([K.count_params(p) for p in model.non_trainable_weights])

    number_size = 4.0
    if K.floatx() == 'float16':
        number_size = 2.0
    if K.floatx() == 'float64':
        number_size = 8.0

    total_memory = number_size * (batch_size * shapes_mem_count + trainable_count + non_trainable_count)
    gbytes = np.round(total_memory / (1024.0 ** 3), 3) + internal_model_mem_count
    return gbytes